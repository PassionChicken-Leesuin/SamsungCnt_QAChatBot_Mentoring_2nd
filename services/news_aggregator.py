# services/news_aggregator.py

import pandas as pd

import fetchers.google_rss as google_rss
import fetchers.naver_news as naver_news
import fetchers.kakao_news as kakao_news
import fetchers.tavily as tavily
import fetchers.serpapi as serpapi


def _normalize_common_columns(df: pd.DataFrame, source: str) -> pd.DataFrame:
    """
    ê° ì†ŒìŠ¤ë³„ë¡œ ì œê°ê°ì¸ ì»¬ëŸ¼ëª…ì„ ìµœëŒ€í•œ í†µì¼í•˜ê³ ,
    ë¶ˆí•„ìš”í•œ link ì»¬ëŸ¼ ë“±ì„ ì •ë¦¬í•˜ëŠ” í—¬í¼ í•¨ìˆ˜.
    - URL: url
    - ë‚ ì§œ: published
    - ìš”ì•½(ìžˆìœ¼ë©´): summary

    âš  ì¼ë¶€ fetcher(ì˜ˆ: SerpAPI)ëŠ” ì´ë¯¸ 'published'ë¥¼ datetimeìœ¼ë¡œ ë§Œë“¤ì–´ì„œ ì£¼ë¯€ë¡œ,
      ì—¬ê¸°ì„œëŠ” 'published'ê°€ ì´ë¯¸ ìžˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ë‘ê³  ì¶”ê°€ rename/drop ì •ë„ë§Œ ìˆ˜í–‰.
    """
    if df is None or df.empty:
        return pd.DataFrame()

    df = df.copy()
    df["source"] = source

    # ---------- URL ì»¬ëŸ¼ í†µì¼ ----------
    # ìš°ì„ ìˆœìœ„: url > link > linkUrl ...
    if "url" not in df.columns:
        if "link" in df.columns:
            df = df.rename(columns={"link": "url"})
        elif "linkUrl" in df.columns:
            df = df.rename(columns={"linkUrl": "url"})

    # urlì´ ìƒê²¼ëŠ”ë° linkê°€ ë‚¨ì•„ ìžˆìœ¼ë©´ ì‚­ì œ
    if "url" in df.columns and "link" in df.columns:
        df = df.drop(columns=["link"])

    # ---------- ë‚ ì§œ ì»¬ëŸ¼ í†µì¼ ----------
    # ì´ë¯¸ fetcherì—ì„œ 'published'ë¥¼ ë§Œë“¤ì–´ ì¤¬ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë‘ê³ ,
    # pubDate / datetime / date ê°™ì€ raw ì»¬ëŸ¼ì€ ì •ë¦¬ë§Œ í•´ì¤€ë‹¤.
    if "published" in df.columns:
        # ì›ì‹œ ë‚ ì§œ ì»¬ëŸ¼ì€ ìžˆìœ¼ë©´ ì •ë¦¬ (ì„ íƒ ì‚¬í•­)
        for c in ["pubDate", "datetime", "date"]:
            if c in df.columns:
                df = df.drop(columns=[c])
    else:
        # publishedê°€ ì—†ìœ¼ë©´ pubDate / datetime / date ì¤‘ í•˜ë‚˜ë¥¼ publishedë¡œ ìŠ¹ê²©
        for c in ["pubDate", "datetime", "date"]:
            if c in df.columns:
                df = df.rename(columns={c: "published"})
                break

    # ---------- ìš”ì•½ ì»¬ëŸ¼ í†µì¼ (ìžˆìœ¼ë©´) ----------
    # Google RSSëŠ” ê¸°ì¡´ ë¡œì§ëŒ€ë¡œ description -> summary
    if "description" in df.columns and "summary" not in df.columns:
        df = df.rename(columns={"description": "summary"})

    return df


def collect_news(
    query,
    sources,
    max_items: int = 10,
    start_date=None,   # ðŸ”¹ ê¸°ê°„ í•„í„° ì‹œìž‘ì¼ (date/datetime/str ëª¨ë‘ í—ˆìš©)
    end_date=None,     # ðŸ”¹ ê¸°ê°„ í•„í„° ì¢…ë£Œì¼
) -> pd.DataFrame:
    """
    ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ê³  í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ í•©ì¹œë‹¤.
    - ê° ì†ŒìŠ¤ë³„ë¡œ ìµœëŒ€ max_itemsê°œê¹Œì§€ ìˆ˜ì§‘
    - ê³µí†µ ì»¬ëŸ¼:
        * source   : ë‰´ìŠ¤ ì¶œì²˜ (Google / Naver / Kakao / Tavily / SerpAPI)
        * url      : ê¸°ì‚¬ URL
        * published: ë°œí–‰ì¼(ê°€ëŠ¥í•œ ê²½ìš°)
        * summary  : (ì¼ë¶€ ì†ŒìŠ¤ì—ì„œ ì œê³µí•˜ëŠ” ìš”ì•½/description)
    - start_date, end_dateê°€ ì£¼ì–´ì§€ë©´ published ê¸°ì¤€ìœ¼ë¡œ ê¸°ê°„ í•„í„°ë§
    """
    dfs = []

    # ---------------- Google RSS ----------------
    if "Google" in sources:
        try:
            g_df = google_rss.fetch_google_news_rss(query, max_items=max_items)
            g_df = _normalize_common_columns(g_df, "Google")
            dfs.append(g_df)
        except Exception as e:
            print(f"[Google] ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")

    # ---------------- Naver ----------------
    if "Naver" in sources:
        try:
            data = naver_news.search_naver_news(query, display=max_items)
            n_df = naver_news.naver_news_to_df(data)
            n_df = _normalize_common_columns(n_df, "Naver")
            dfs.append(n_df)
        except Exception as e:
            print(f"[Naver] ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")

    # ---------------- Kakao ----------------
    if "Kakao" in sources:
        try:
            data = kakao_news.fetch_kakao_web(query, size=max_items)
            k_df = kakao_news.kakao_web_to_df(data)
            k_df = _normalize_common_columns(k_df, "Kakao")
            dfs.append(k_df)
        except Exception as e:
            print(f"[Kakao] ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")

    # ---------------- Tavily ----------------
    if "Tavily" in sources:
        try:
            data = tavily.fetch_tavily_news(query, max_results=max_items)
            t_df = tavily.tavily_to_df(data)
            t_df = _normalize_common_columns(t_df, "Tavily")
            dfs.append(t_df)
        except Exception as e:
            print(f"[Tavily] ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")

    # ---------------- SerpAPI ----------------
    if "SerpAPI" in sources:
        try:
            data = serpapi.fetch_serpapi_news(query, num=max_items)
            s_df = serpapi.serpapi_to_df(data)
            s_df = _normalize_common_columns(s_df, "SerpAPI")
            dfs.append(s_df)
        except Exception as e:
            print(f"[SerpAPI] ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")

    # ì•„ë¬´ ì†ŒìŠ¤ë„ ì„±ê³µì ìœ¼ë¡œ ìˆ˜ì§‘ ëª»í•œ ê²½ìš°
    if not dfs:
        return pd.DataFrame()

    # í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
    all_df = pd.concat(dfs, ignore_index=True, sort=False)

    # ì „ì²´ê°€ NaNì¸ ì»¬ëŸ¼ì€ ì œê±° (ìž¡ìŠ¤ëŸ¬ìš´ ì»¬ëŸ¼ ì •ë¦¬)
    all_df = all_df.dropna(axis=1, how="all")

    # ==================================================
    # ðŸ”¹ ê¸°ê°„ í•„í„°ë§ (start_date / end_dateê°€ ì£¼ì–´ì¡Œì„ ë•Œ)
    #   - publishedë¥¼ ë‚ ì§œ(date) ë‹¨ìœ„ë¡œë§Œ ë¹„êµ (ì‹œê°„/íƒ€ìž„ì¡´ì€ ë¬´ì‹œ)
    #   - ë‚ ì§œê°€ ì—†ëŠ”(NaT) ê¸°ì‚¬ë“¤ì€ í•­ìƒ í¬í•¨
    # ==================================================
    if (start_date is not None or end_date is not None) and ("published" in all_df.columns):
        # 1) publishedë¥¼ datetimeìœ¼ë¡œ ì¼ë‹¨ í†µì¼ ì‹œë„
        if not pd.api.types.is_datetime64_any_dtype(all_df["published"]):
            all_df["published"] = pd.to_datetime(all_df["published"], errors="coerce")

        # 2) ì—¬ì „ížˆ datetime íƒ€ìž…ì´ ì•„ë‹ˆë©´, í•„í„°ë§ í¬ê¸°í•˜ê³  ê·¸ëŒ€ë¡œ ë°˜í™˜
        if not pd.api.types.is_datetime64_any_dtype(all_df["published"]):
            return all_df

        pub = all_df["published"]
        # ðŸ”¹ ë‚ ì§œë§Œ ë½‘ê¸° (YYYY-MM-DD)
        pub_date = pub.dt.date
        has_date = pub.notna()
        no_date = pub.isna()

        # 3) ê²½ê³„ê°’ë„ date íƒ€ìž…ìœ¼ë¡œ ë§žì¶”ê¸° (í˜¹ì‹œ datetime/Timestampë¡œ ë“¤ì–´ì˜¤ëŠ” ê²½ìš° ëŒ€ë¹„)
        from datetime import datetime as _dt
        from pandas import Timestamp as _Ts

        sd = start_date
        ed = end_date

        if isinstance(sd, (_dt, _Ts)):
            sd = sd.date()
        if isinstance(ed, (_dt, _Ts)):
            ed = ed.date()

        # 4) ì¡°ê±´ ë§Œë“¤ê¸° (ë‚ ì§œ ìžˆëŠ” ì• ë“¤ë§Œ ë²”ìœ„ ë¹„êµ, ë‚ ì§œ ì—†ëŠ” ì• ë“¤ì€ ë¬´ì¡°ê±´ í¬í•¨)
        cond = pd.Series(True, index=all_df.index)

        if sd is not None:
            cond &= pub_date >= sd
        if ed is not None:
            cond &= pub_date <= ed

        mask = no_date | (has_date & cond)
        all_df = all_df[mask]

    return all_df

