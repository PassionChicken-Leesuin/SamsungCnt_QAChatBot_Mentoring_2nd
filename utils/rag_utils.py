# utils/rag_utils.py

from typing import Dict, List, Tuple
from copy import deepcopy

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import load_prompt, ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableLambda, RunnableMap


# -----------------------------
# 1) ê³µí†µ ìœ í‹¸ / ìƒìˆ˜ë“¤
# -----------------------------
def normalize_str(x):
    return str(x or "").strip()


REGION_CANDIDATES = [
    "ì„œìš¸", "ë¶€ì‚°", "ëŒ€êµ¬", "ì¸ì²œ", "ê´‘ì£¼", "ëŒ€ì „", "ìš¸ì‚°", "ì„¸ì¢…",
    "ê²½ê¸°", "ê°•ì›", "ì¶©ë¶", "ì¶©ë‚¨", "ì „ë¶", "ì „ë‚¨", "ê²½ë¶", "ê²½ë‚¨", "ì œì£¼"
]

FORKLIFT_KEYWORDS = ["ì§€ê²Œì°¨", "í¬í¬ë¦¬í”„íŠ¸", "forklift"]

EQUIPMENT_CATEGORIES = {
    "ì§€ê²Œì°¨": ["ì§€ê²Œì°¨", "í¬í¬ë¦¬í”„íŠ¸", "forklift"],
    "í¬ë ˆì¸": ["í¬ë ˆì¸", "ì²œì¥í¬ë ˆì¸", "ì²œì •í¬ë ˆì¸", "ì´ë™ì‹í¬ë ˆì¸", "ì´ë™ì‹ í¬ë ˆì¸"],
    "íƒ€ì›Œí¬ë ˆì¸": ["íƒ€ì›Œí¬ë ˆì¸", "íƒ€ì›Œ í¬ë ˆì¸"],
    "êµ´ì‚­ê¸°": ["êµ´ì‚­ê¸°", "í¬í¬ë ˆì¸", "í¬í´ë ˆì¸", "ë°±í˜¸", "excavator", "backhoe"],
    "ë³€ì••ê¸°": ["ë³€ì••ê¸°", "ë³€ì „ì‹¤", "íŠ¸ëœìŠ¤", "ë³€ì „ì†Œ"],
    "íŠ¸ëŸ­/ì°¨ëŸ‰": ["íŠ¸ëŸ­", "í™”ë¬¼ì°¨", "ìŠ¹ìš©ì°¨", "ì°¨ëŸ‰", "ë¤í”„íŠ¸ëŸ­", "ì§€ê²Œì°¨ìš´í–‰", "ì°¨ëŸ‰ìš´í–‰", "í›„ì§„ì¤‘"],
    "ì»¨ë² ì´ì–´": ["ì»¨ë² ì´ì–´", "conveyor"],
    "í”„ë ˆìŠ¤": ["í”„ë ˆìŠ¤", "press"],
    "ê¸°ê³„í†±": ["ê¸°ê³„í†±", "ì „ê¸°í†±", "ì²´ì¸í†±", "ì „ë™í†±"],
    "ì‚¬ë‹¤ë¦¬": ["ì‚¬ë‹¤ë¦¬"],
    "ë¦¬í”„íŠ¸": ["ë¦¬í”„íŠ¸", "ê³ ì†Œì‘ì—…ëŒ€"],
    "í˜¸ì´ìŠ¤íŠ¸": ["í˜¸ì´ìŠ¤íŠ¸"],
    "ë¡œí”„/ì™€ì´ì–´": ["ì™€ì´ì–´ë¡œí”„", "ì™€ì´ì–´ ë¡œí”„", "ë¡œí”„"],
    "ì••ì¶•ê¸°": ["ì••ì¶•ê¸°", "compressor"],
}

ACCIDENT_CATEGORY_DEFS = {
    "ê¸°ê³„ì‚¬ê³ ": {
        "triggers": ["ê¸°ê³„ì‚¬ê³ ", "ê¸°ê³„ ì‚¬ê³ "],
        "patterns": ["ê¸°ê³„", "ê¸°ê³„ê¸°êµ¬", "ê¸°ê³„Â·ê¸°êµ¬", "ê¸°ê³„ã†ê¸°êµ¬", "í”„ë ˆìŠ¤", "ì ˆë‹¨ê¸°", "ì „ë‹¨ê¸°", "press"],
    },
    "ì „ê¸°ì‚¬ê³ ": {
        "triggers": ["ì „ê¸°ì‚¬ê³ ", "ì „ê¸° ì‚¬ê³ ", "ê°ì „ì‚¬ê³ ", "ê°ì „ ì‚¬ê³ ", "ë³€ì••ê¸° ì‚¬ê³ "],
        "patterns": ["ì „ê¸°", "ê°ì „", "ë³€ì••ê¸°", "ë³€ì „ì‹¤", "ê³ ì••", "ëˆ„ì „", "ì „ì„ ", "ë¶„ì „ë°˜", "ì°¨ë‹¨ê¸°"],
    },
    "êµí†µì‚¬ê³ ": {
        "triggers": ["êµí†µì‚¬ê³ ", "êµí†µ ì‚¬ê³ "],
        "patterns": ["êµí†µì‚¬ê³ ", "ì°¨ëŸ‰", "íŠ¸ëŸ­", "í™”ë¬¼ì°¨", "ìŠ¹ìš©ì°¨", "ë„ë¡œ", "ì£¼í–‰", "ìš´í–‰ì¤‘", "ìš´í–‰ ì¤‘", "í›„ì§„ì¤‘", "í›„ì§„ ì¤‘"],
    },
    "í™”í•™ë¬¼ì§ˆ ë…¸ì¶œ ì‚¬ê³ ": {
        "triggers": ["í™”í•™ë¬¼ì§ˆ ë…¸ì¶œ ì‚¬ê³ ", "í™”í•™ë¬¼ì§ˆ ë…¸ì¶œì‚¬ê³ ", "í™”í•™ë¬¼ì§ˆë…¸ì¶œì‚¬ê³ ", "í™”í•™ë¬¼ì§ˆ ì‚¬ê³ ", "í™”í•™ì‚¬ê³ "],
        "patterns": [
            "í™”í•™ë¬¼ì§ˆ", "í™”í•™", "ìœ í•´ë¬¼ì§ˆ", "ìœ ë…ë¬¼",
            "ê°€ìŠ¤ëˆ„ì¶œ", "ê°€ìŠ¤ ëˆ„ì¶œ", "ìœ í•´ê°€ìŠ¤", "ìœ ë…ê°€ìŠ¤",
            "ì¤‘ë…", "í¡ì…", "ì§ˆì‹", "ëˆ„ì¶œ"
        ],
    },
    "í¬ë ˆì¸ ì‚¬ê³ ": {
        "triggers": ["í¬ë ˆì¸ì‚¬ê³ ", "í¬ë ˆì¸ ì‚¬ê³ "],
        "patterns": ["í¬ë ˆì¸", "ì²œì¥í¬ë ˆì¸", "ì²œì •í¬ë ˆì¸", "ì´ë™ì‹í¬ë ˆì¸", "ì´ë™ì‹ í¬ë ˆì¸"],
    },
    "íƒ€ì›Œí¬ë ˆì¸ ì‚¬ê³ ": {
        "triggers": ["íƒ€ì›Œí¬ë ˆì¸ì‚¬ê³ ", "íƒ€ì›Œí¬ë ˆì¸ ì‚¬ê³ ", "íƒ€ì›Œ í¬ë ˆì¸ ì‚¬ê³ "],
        "patterns": ["íƒ€ì›Œí¬ë ˆì¸", "íƒ€ì›Œ í¬ë ˆì¸"],
    },
    "êµ´ì‚­ê¸° ì‚¬ê³ ": {
        "triggers": ["êµ´ì‚­ê¸°ì‚¬ê³ ", "êµ´ì‚­ê¸° ì‚¬ê³ ", "í¬í¬ë ˆì¸ ì‚¬ê³ ", "í¬í´ë ˆì¸ ì‚¬ê³ "],
        "patterns": ["êµ´ì‚­ê¸°", "í¬í¬ë ˆì¸", "í¬í´ë ˆì¸", "ë°±í˜¸", "excavator", "backhoe"],
    },
    "ë³€ì••ê¸° ì‚¬ê³ ": {
        "triggers": ["ë³€ì••ê¸°ì‚¬ê³ ", "ë³€ì••ê¸° ì‚¬ê³ "],
        "patterns": ["ë³€ì••ê¸°", "ë³€ì „ì‹¤", "ë³€ì „ì†Œ", "íŠ¸ëœìŠ¤"],
    },
}


# -----------------------------
# 2) í•„í„°/í†µê³„ ê´€ë ¨ ìœ í‹¸
# -----------------------------
def parse_filters_from_question(question: str, known_types=None) -> Dict:
    import re

    q_raw = question
    q = question.replace(" ", "")
    filters = {}

    # ì—°ë„
    m = re.search(r"(\d{4})ë…„", q)
    if m:
        filters["year"] = m.group(1)

    # ì§€ì—­
    for r in REGION_CANDIDATES:
        if r in q:
            filters["region"] = r
            break

    # ì‚¬ê³ ìœ í˜•
    types = []
    if known_types:
        for t in known_types:
            t_norm = str(t).strip()
            if t_norm and t_norm in q_raw:
                types.append(t_norm)

    common_type_words = [
        "ë¶•ê´´", "ì¶”ë½", "í˜‘ì°©", "ë¼ì„", "ë‚™í•˜",
        "ì¶©ëŒ", "ì „ë„", "ê°ì „", "í­ë°œ", "í™”ì¬", "ì§ˆì‹", "ì¤‘ë…"
    ]
    for w in common_type_words:
        if w in q_raw and w not in types:
            types.append(w)

    # ì¥ë¹„ í‚¤ì›Œë“œ
    kw_list = []
    for kw in FORKLIFT_KEYWORDS:
        if kw in q_raw:
            kw_list.append(kw)

    # ìƒìœ„ ì¹´í…Œê³ ë¦¬
    for cat_name, defs in ACCIDENT_CATEGORY_DEFS.items():
        triggers = defs.get("triggers", [])
        patterns = defs.get("patterns", [])
        if any(tr in q_raw for tr in triggers):
            types.extend(patterns)
            kw_list.extend(patterns)

    if types:
        filters["types"] = list(dict.fromkeys(types))
    if kw_list:
        filters["keywords"] = list(dict.fromkeys(kw_list))

    return filters


def doc_matches_filters(doc, filters: Dict) -> bool:
    if not filters:
        return True

    meta = getattr(doc, "metadata", {}) or {}
    text = normalize_str(getattr(doc, "page_content", ""))

    meta_occ_date = normalize_str(meta.get("occurrence_date", "") or meta.get("ë°œìƒì¼ì", ""))
    meta_region = normalize_str(meta.get("region_raw", "") or meta.get("ì§€ì—­", "") or meta.get("location", ""))
    meta_type = normalize_str(meta.get("accident_type", "") or meta.get("ì‚¬ê³ ìœ í˜•", ""))
    meta_keyword = normalize_str(meta.get("keyword", ""))
    meta_title = normalize_str(meta.get("title", ""))
    meta_contents = normalize_str(meta.get("contents", ""))
    meta_text_embed = normalize_str(meta.get("text_for_embedding", "") or meta.get("text", ""))

    # ì—°ë„
    year = filters.get("year")
    if year:
        year_ok = False
        if year in meta_occ_date or year in text or year in meta_contents or year in meta_text_embed:
            year_ok = True
        if not year_ok:
            return False

    # ì§€ì—­
    region = filters.get("region")
    if region:
        region_ok = False
        if region in meta_region or region in text or region in meta_contents or region in meta_text_embed:
            region_ok = True
        if not region_ok:
            return False

    # ì‚¬ê³ ìœ í˜•
    types = filters.get("types")
    if types:
        type_ok = False
        for t in types:
            if (
                t in meta_type
                or t in meta_keyword
                or t in meta_title
                or t in meta_contents
                or t in meta_text_embed
                or t in text
            ):
                type_ok = True
                break
        if not type_ok:
            return False

    # ì¥ë¹„ í‚¤ì›Œë“œ
    keywords = filters.get("keywords")
    if keywords:
        kw_ok = False
        for kw in keywords:
            if (
                kw in meta_keyword
                or kw in meta_title
                or kw in meta_contents
                or kw in meta_text_embed
                or kw in text
            ):
                kw_ok = True
                break
        if not kw_ok:
            return False

    return True


def count_matching_accidents(filters: Dict, docs: List) -> int:
    if not filters:
        return 0

    accident_ids = set()
    no_id_count = 0

    for idx, d in enumerate(docs):
        if not doc_matches_filters(d, filters):
            continue

        meta = getattr(d, "metadata", {}) or {}
        acc_id = meta.get("accident_id")
        if acc_id:
            accident_ids.add(str(acc_id))
        else:
            no_id_count += 1

    if accident_ids:
        return len(accident_ids)
    else:
        return no_id_count


def compute_equipment_stats(docs: List, base_filters: Dict = None) -> Dict[str, int]:
    if base_filters is None:
        base_filters = {}
    else:
        base_filters = deepcopy(base_filters)
        if "keywords" in base_filters:
            base_filters.pop("keywords")

    equip_to_accidents = {label: set() for label in EQUIPMENT_CATEGORIES.keys()}

    for idx, d in enumerate(docs):
        if not doc_matches_filters(d, base_filters):
            continue

        meta = getattr(d, "metadata", {}) or {}
        acc_id = meta.get("accident_id")
        if acc_id:
            acc_key = str(acc_id)
        else:
            acc_key = f"doc_{idx}"

        text = normalize_str(getattr(d, "page_content", ""))
        meta_keyword = normalize_str(meta.get("keyword", ""))
        meta_title = normalize_str(meta.get("title", ""))
        meta_contents = normalize_str(meta.get("contents", ""))
        meta_text_embed = normalize_str(meta.get("text_for_embedding", "") or meta.get("text", ""))

        combined = " ".join([text, meta_keyword, meta_title, meta_contents, meta_text_embed])

        for label, patterns in EQUIPMENT_CATEGORIES.items():
            for p in patterns:
                if p and p in combined:
                    equip_to_accidents[label].add(acc_key)
                    break

    result = {label: len(acc_ids) for label, acc_ids in equip_to_accidents.items() if acc_ids}
    return result


# -----------------------------
# 3) ë²¡í„°ìŠ¤í† ì–´ ë¡œë”© + í†µê³„ ìƒì„±
# -----------------------------
def load_vectorstore(
    embedding_model: str,
    index_path: str = "faiss_index"
) -> Tuple[FAISS, List]:
    """
    Streamlitì— ì˜ì¡´í•˜ì§€ ì•ŠëŠ” ìˆœìˆ˜ ë¡œë”© í•¨ìˆ˜.
    """
    store = FAISS.load_local(
        folder_path=index_path,
        embeddings=OpenAIEmbeddings(model=embedding_model),
        allow_dangerous_deserialization=True,
    )

    docstore = store.docstore
    if hasattr(docstore, "_dict"):
        docs = list(docstore._dict.values())
    else:
        docs = []

    return store, docs


def build_accident_stats(docs: List) -> Dict:
    """
    ê¸°ì¡´ load_vectorstore_and_stats ì•ˆì—ì„œ í•˜ë˜ í†µê³„ ê³„ì‚°ë§Œ ë¶„ë¦¬
    """
    total_docs = len(docs)
    accident_ids = set()
    accident_type_labels = set()

    for d in docs:
        meta = getattr(d, "metadata", {}) or {}
        acc_id = meta.get("accident_id")
        if acc_id:
            accident_ids.add(str(acc_id))

        acc_type = meta.get("accident_type")
        if acc_type:
            accident_type_labels.add(str(acc_type).strip())

    if accident_ids:
        total_accidents = len(accident_ids)
    else:
        total_accidents = total_docs

    stats = {
        "total_docs": total_docs,
        "total_accidents": total_accidents,
        "accident_type_labels": sorted(accident_type_labels),
    }
    return stats


def answer_with_stats_using_index(
    question: str,
    stats: Dict,
    docs: List,
) -> str | None:
    """
    ê¸°ì¡´ try_answer_with_stats_using_index ê·¸ëŒ€ë¡œ ì˜®ê²¨ë‘” ë²„ì „.
    í†µê³„ ì§ˆë¬¸ì´ ì•„ë‹ˆë©´ None ë¦¬í„´.
    """
    q_raw = question
    q = question.replace(" ", "").strip()

    is_count_query = (
        ("ì‚¬ê³ " in q or "ì‚¬ë¡€" in q or "ì¬í•´" in q)
        and (
            "ê±´ìˆ˜" in q or "ëª‡ê±´" in q or "ëª‡ê°œ" in q or
            "ëª‡ê±´ì´ì•¼" in q or "ëª‡ê°œì•¼" in q or "ì–¼ë§ˆë‚˜" in q or "í†µê³„" in q
        )
    )

    if not is_count_query:
        return None

    known_types = stats.get("accident_type_labels", [])
    filters = parse_filters_from_question(q_raw, known_types=known_types)

    # ì¥ë¹„ë³„ í†µê³„?
    is_equipment_summary_query = (
        ("ì¥ë¹„" in q or "ì„¤ë¹„" in q)
        and ("ë³„" in q or "ì¢…ë¥˜" in q or "ë¶„ë¥˜" in q or "í†µê³„" in q)
    )

    if is_equipment_summary_query:
        base_filters = {k: v for k, v in filters.items() if k in ("year", "region", "types")}
        equip_stats = compute_equipment_stats(docs, base_filters=base_filters)

        if not equip_stats:
            return "í˜„ì¬ ì¡°ê±´ì— í•´ë‹¹í•˜ëŠ” ì¥ë¹„ë³„ ì‚¬ê³  ì‚¬ë¡€ë¥¼ ì¸ë±ìŠ¤ì—ì„œ ì°¾ê¸° ì–´ë µìŠµë‹ˆë‹¤."

        sorted_items = sorted(equip_stats.items(), key=lambda x: x[1], reverse=True)

        lines = []
        for label, cnt in sorted_items[:8]:
            lines.append(f"- {label}: {cnt:,}ê±´")

        cond_parts = []
        if "year" in base_filters:
            cond_parts.append(f"{base_filters['year']}ë…„")
        if "region" in base_filters:
            cond_parts.append(base_filters["region"])
        if "types" in base_filters:
            cond_parts.append("/".join(base_filters["types"]) + " ì‚¬ê³ ")

        if cond_parts:
            cond_text = " / ".join(cond_parts)
            title = f"í˜„ì¬ ë²¡í„° ì¸ë±ìŠ¤ ê¸°ì¤€, **{cond_text} ì¥ë¹„ë³„ ì‚¬ê³  ê±´ìˆ˜**ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n\n"
        else:
            title = "í˜„ì¬ ë²¡í„° ì¸ë±ìŠ¤ ê¸°ì¤€, **ì¥ë¹„ë³„ ì‚¬ê³  ê±´ìˆ˜**ëŠ” ëŒ€ëµ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n\n"

        return title + "\n".join(lines)

    # ì¼ë°˜ ê±´ìˆ˜ ì§ˆë¬¸
    if not filters:
        total_acc = stats.get("total_accidents")
        total_docs = stats.get("total_docs")
        return (
            f"í˜„ì¬ ì´ ì±—ë´‡ì´ ì°¸ì¡°í•˜ëŠ” ë²¡í„° ì¸ë±ìŠ¤ ê¸°ì¤€ìœ¼ë¡œëŠ” "
            f"**ì´ {total_acc:,}ê±´ì˜ ì‚¬ê³  ì‚¬ë¡€**ê°€ ìˆìŠµë‹ˆë‹¤.\n\n"
            f"(ì¸ë±ìŠ¤ì—ëŠ” ì´ {total_docs:,}ê°œì˜ ë¬¸ì„œê°€ ì €ì¥ë˜ì–´ ìˆìœ¼ë©°, "
            f"ì—¬ëŸ¬ ë¬¸ì„œê°€ í•˜ë‚˜ì˜ ì‚¬ê³  ì‚¬ë¡€ì— ëŒ€ì‘ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.)"
        )

    count = count_matching_accidents(filters, docs)

    parts = []
    if "year" in filters:
        parts.append(f"{filters['year']}ë…„")
    if "region" in filters:
        parts.append(filters["region"])
    if "types" in filters:
        type_label = "/".join(filters["types"])
        parts.append(f"{type_label} ì‚¬ê³ ")
    if "keywords" in filters:
        kw_label = "/".join(filters["keywords"])
        parts.append(f"{kw_label} ê´€ë ¨")

    if not parts:
        prefix = "í•´ë‹¹ ì¡°ê±´"
    else:
        prefix = " ".join(parts)

    return (
        f"í˜„ì¬ ë²¡í„° ì¸ë±ìŠ¤ ê¸°ì¤€ìœ¼ë¡œ, **{prefix} ì‚¬ê³  ì‚¬ë¡€ëŠ” ì•½ {count:,}ê±´**ì…ë‹ˆë‹¤.\n\n"
        f"(ì´ ìˆ«ìëŠ” ì¸ë±ìŠ¤ì— í¬í•¨ëœ ë°ì´í„° ê¸°ì¤€ì´ë©°, ì›ë³¸ ì „ì²´ ë°ì´í„° ê±´ìˆ˜ì™€ëŠ” ì°¨ì´ê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.)"
    )


# -----------------------------
# 4) RAG ê²€ìƒ‰ ê´€ë ¨ ìœ í‹¸
# -----------------------------
def build_metadata_filter(query: str) -> Dict:
    """
    ì›ë˜ build_metadata_filter ê·¸ëŒ€ë¡œ
    """
    filters = {}

    accident_keywords = {
        "ê°ì „": "ê°ì „", "ì „ê¸°": "ê°ì „",
        "í™”ì¬": "í™”ì¬",
        "ì¶”ë½": "ì¶”ë½",
        "í˜‘ì°©": "í˜‘ì°©",
        "ì§ˆì‹": "ì§ˆì‹",
        "ì¤‘ë…": "ì¤‘ë…", "ê°€ìŠ¤": "ì¤‘ë…",
    }

    for k, v in accident_keywords.items():
        if k in query:
            filters["accident_type"] = v

    if any(w in query for w in ["ì‘ê¸‰", "ì¡°ì¹˜", "ëŒ€ì²˜"]):
        filters["section"] = "ì‘ê¸‰ì¡°ì¹˜"
    if "ì˜ˆë°©" in query:
        filters["section"] = "ì˜ˆë°©"
    if any(w in query for w in ["ë²•", "ê·œì •"]):
        filters["section"] = "ë²•ê·œ"

    return filters


def format_docs(docs: List) -> str:
    formatted = []
    for i, d in enumerate(docs):
        md = d.metadata
        header = (
            f"[ë¬¸ì„œ {i+1}] "
            f"(type: {md.get('accident_type','N/A')}, "
            f"section: {md.get('section','N/A')}, "
            f"source: {md.get('source','N/A')}, "
            f"page: {md.get('page_num','?')})"
        )
        formatted.append(header + "\n" + d.page_content)
    return "\n\n".join(formatted)


def create_rag_chain(
    vectorstore,
    rag_prompt_path: str = "prompts/first.yaml",
    llm_model_name: str = "gpt-4o-mini",
):
    """
    RAG ì²´ì¸ ìƒì„± í•¨ìˆ˜.

    - ì™¸ë¶€ ì…ë ¥: {"question": str, "chat_history": str}
    - rag_prompt(input_variables): ["context", "question", "chat_history"]
      (ì§€ê¸ˆ ë„¤ê°€ ì œê³µí•œ history í”„ë¡¬í”„íŠ¸ í˜•íƒœ)
    """
    rag_prompt = load_prompt(rag_prompt_path, encoding="utf-8")
    llm = ChatOpenAI(model=llm_model_name, temperature=0)

    # â¬‡ ê²€ìƒ‰ìš© ì§ˆë¬¸ ì¬ì‘ì„± í”„ë¡¬í”„íŠ¸ (ì—¬ê¸°ì„œëŠ” history ë¼ëŠ” ì´ë¦„ì„ ë‚´ë¶€ìš©ìœ¼ë¡œë§Œ ì‚¬ìš©)
    condense_prompt = ChatPromptTemplate.from_template(
        """
        ë„ˆëŠ” ê²€ìƒ‰ìš© ì§ˆë¬¸ì„ ì¬ì‘ì„±í•˜ëŠ” ë³´ì¡° ë„ìš°ë¯¸ì´ë‹¤.

        ì•„ë˜ ëŒ€í™” ì´ë ¥ì„ ì°¸ê³ í•˜ì—¬, ì‚¬ìš©ìì˜ ë§ˆì§€ë§‰ ì§ˆë¬¸ì„
        ê²€ìƒ‰ ì—”ì§„ì— ë„£ì„ **í•œ ê°œì˜ ë…ë¦½ì ì¸ ì§ˆë¬¸**ìœ¼ë¡œ ë°”ê¿”ë¼.

        - ì›ë˜ ì§ˆë¬¸ì— ë“±ì¥í•˜ëŠ” í•µì‹¬ í‚¤ì›Œë“œ(ì¥ë¹„ëª…, ë²•ê·œëª…, ì‚¬ê³ ìœ í˜• ë“±)ëŠ” ì ˆëŒ€ ì‚­ì œí•˜ì§€ ë§ˆë¼.
        - 'ê·¸ ì‚¬ê³ ', 'ì´ ì‚¬ë¡€', 'ìœ„ì—ì„œ ë§í•œ ê²ƒ' ê°™ì€ í‘œí˜„ì€
          ëŒ€í™” ì´ë ¥ì— ë“±ì¥í•œ êµ¬ì²´ì ì¸ ë‚´ìš©ìœ¼ë¡œ í’€ì–´ì¨ë¼.
        - ìƒˆë¡œìš´ ì‚¬ì‹¤ì„ ì§€ì–´ë‚´ì§€ ë§ê³ , ëŒ€í™”ì— ë‚˜ì˜¨ ì •ë³´ë§Œ ì‚¬ìš©í•´ë¼.

        [ëŒ€í™” ì´ë ¥]
        {history}

        [ì‚¬ìš©ìì˜ ë§ˆì§€ë§‰ ì§ˆë¬¸]
        {question}

        [ì¬ì‘ì„±ëœ ê²€ìƒ‰ìš© ì§ˆë¬¸]
        """
    )
    condense_chain = condense_prompt | llm | StrOutputParser()

    # ğŸ”¹ inputs: {"question": ..., "chat_history": ...}
    def retrieve_with_dual_search(inputs):
        query = inputs["question"]
        # âœ… ì™¸ë¶€ì—ì„œ ë“¤ì–´ì˜¤ëŠ” íˆìŠ¤í† ë¦¬ í‚¤ ì´ë¦„ì€ chat_history
        history = inputs.get("chat_history", "")

        metadata_filter = build_metadata_filter(query)

        retriever = vectorstore.as_retriever(
            search_type="mmr",
            search_kwargs={
                "k": 6,
                "fetch_k": 20,
                "lambda_mult": 0.4,
                "filter": metadata_filter if metadata_filter else None,
            },
        )

        # 1) ì›ë˜ ì§ˆë¬¸ìœ¼ë¡œ ê²€ìƒ‰
        docs_original = retriever.invoke(query)

        # 2) ëŒ€í™” ì´ë ¥ì´ ìˆìœ¼ë©´, ì¬ì‘ì„±ëœ ì§ˆë¬¸ìœ¼ë¡œ í•œ ë²ˆ ë” ê²€ìƒ‰
        docs_contextual = []
        if history.strip():
            rewritten = condense_chain.invoke(
                {"history": history, "question": query}
            ).strip()
            if rewritten:
                try:
                    docs_contextual = retriever.invoke(rewritten)
                except Exception:
                    docs_contextual = []

        # 3) ë‘ ê²°ê³¼ë¥¼ í•©ì¹˜ê³ , ë‚´ìš© ì¤‘ë³µ ì œê±°
        seen = set()
        final_docs = []
        for d in docs_original + docs_contextual:
            content = d.page_content
            if content not in seen:
                seen.add(content)
                final_docs.append(d)

        # í”„ë¡¬í”„íŠ¸ì— ë“¤ì–´ê°ˆ context ë¬¸ìì—´ë¡œ ë³€í™˜
        return format_docs(final_docs)

    # ğŸ”¹ rag_prompt ê°€ ê¸°ëŒ€í•˜ëŠ” í‚¤:
    #    ["context", "question", "chat_history"]
    chain = (
        RunnableMap(
            {
                "context": RunnableLambda(retrieve_with_dual_search),
                "question": RunnableLambda(lambda x: x["question"]),
                "chat_history": RunnableLambda(lambda x: x.get("chat_history", "")),
            }
        )
        | rag_prompt
        | llm
        | StrOutputParser()
    )

    return chain
